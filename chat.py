import json
import numpy as np
import faiss
import torch
import os
from PIL import Image
from langdetect import detect
from transformers import AutoTokenizer, AutoModel
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI

class ArchaeologicalRAG:
    def __init__(self, json_path):
        """Initialise le systÃ¨me RAG avec FAISS et le modÃ¨le de gÃ©nÃ©ration."""
        self.data_store = []
        self.index = faiss.IndexFlatL2(384)  # 384 dimensions pour MiniLM

        # ModÃ¨le de vectorisation de texte
        self.text_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")
        self.text_model = AutoModel.from_pretrained("sentence-transformers/all-MiniLM-L6-v2")

        # ModÃ¨le de gÃ©nÃ©ration de texte
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            google_api_key="AIzaSyDPNekCR-fsWRGFXRXIzBubcyUvY5KKOLQ",  # Remplacez par votre clÃ© API
            temperature=0.3
        )

        # Charger et indexer les donnÃ©es JSON
        self.load_json_data(json_path)

    def load_json_data(self, json_path):
        """Charge les donnÃ©es JSON et les indexe dans FAISS"""
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        for entry in data:
            text = entry.get("description", "").strip()
            if not text:
                continue  # Ignorer les entrÃ©es sans description
            
            text_embedding = self._vectorize_text(text)

            # Stocker les donnÃ©es
            self.data_store.append({
                "site_number": entry.get("site_number", ""),
                "location": entry.get("location", ""),
                "description": text,
                "images": entry.get("images", [])  # Liste des images liÃ©es
            })

            # Ajouter l'embedding Ã  FAISS
            self.index.add(np.array([text_embedding]))

    def _vectorize_text(self, text):
        """Convertit un texte en vecteur"""
        inputs = self.text_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        with torch.no_grad():
            outputs = self.text_model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).numpy().flatten()

    def detect_language(self, text):
        """DÃ©tecte la langue de la question (franÃ§ais ou anglais)"""
        try:
            lang = detect(text)
            return "fr" if lang == "fr" else "en"
        except:
            return "en"  # Par dÃ©faut, rÃ©pondre en anglais si la dÃ©tection Ã©choue

    def display_images(self, images_list):
        """Affiche les images associÃ©es aux rÃ©sultats"""
        for img_path in images_list:
            if os.path.exists(img_path):  # VÃ©rifier si l'image existe avant de l'afficher
                img = Image.open(img_path)
                img.show()  # Ouvre l'image dans le visualiseur d'images
            else:
                print(f"âš ï¸ Image non trouvÃ©e : {img_path}")

    def query(self, question, k=3):
        """Recherche dans FAISS et gÃ©nÃ¨re une rÃ©ponse en affichant les images associÃ©es"""
        query_embedding = self._vectorize_text(question)

        # Recherche des k meilleurs rÃ©sultats
        distances, indices = self.index.search(np.array([query_embedding]), k)

        # VÃ©rifier si des rÃ©sultats sont retournÃ©s
        valid_indices = [i for i in indices[0] if i >= 0 and i < len(self.data_store)]
        if not valid_indices:
            return "Aucune information archÃ©ologique spÃ©cifique trouvÃ©e sur ce sujet."

        # Construire un contexte dÃ©taillÃ©
        context_entries = [self.data_store[i]["description"] for i in valid_indices]
        context = "\n\n".join(context_entries)

        # RÃ©cupÃ©ration des images associÃ©es aux rÃ©sultats extraits
        images_list = []
        for i in valid_indices:
            images = self.data_store[i].get("images", [])
            if images:
                images_list.extend(images)

        # Affichage des images rÃ©cupÃ©rÃ©es
        if images_list:
            print("\nğŸ“· Images associÃ©es aux rÃ©sultats trouvÃ©s :")
            for img in images_list:
                print(f" - {img}")

            # Afficher les images dans une fenÃªtre
            self.display_images(images_list)

        # DÃ©tection de la langue de la question
        lang = self.detect_language(question)

        # Adapter le prompt pour inclure les images et rÃ©pondre dans la bonne langue
        if lang == "fr":
            prompt_template = """
            [Expert en archÃ©ologie tunisienne]
            Vous Ãªtes un expert en archÃ©ologie et histoire tunisienne. RÃ©pondez de maniÃ¨re dÃ©taillÃ©e et Ã©ducative en intÃ©grant les informations du contexte.

            Contexte archÃ©ologique pertinent :
            {context}

            Images associÃ©es aux vestiges retrouvÃ©s :
            {images}

            Question posÃ©e : {question}

            Fournissez une rÃ©ponse informative en dÃ©taillant les aspects historiques, archÃ©ologiques et culturels :
            """
        else:
            prompt_template = """
            [Expert in Tunisian archaeology]
            You are an expert in Tunisian archaeology and history. Answer in a detailed and educational manner by integrating information from the provided context.

            Relevant archaeological context:
            {context}

            Images associated with the discovered remains:
            {images}

            Question asked: {question}

            Provide an informative response detailing historical, archaeological, and cultural aspects:
            """

        prompt = PromptTemplate.from_template(prompt_template)

        chain = LLMChain(llm=self.llm, prompt=prompt)
        response = chain.run(context=context, images=", ".join(images_list), question=question)

        # Ajouter la liste des images Ã  la fin de la rÃ©ponse
        if images_list:
            response += "\n\nğŸ“Œ **Images associÃ©es Ã  cette analyse :**\n"
            response += "\n".join([f"- {img}" for img in images_list])

        return response

# === ExÃ©cution principale ===
if __name__ == "__main__":
    rag = ArchaeologicalRAG("data.json")  # Remplacez par le chemin de votre fichier JSON


"""
    # Exemple de requÃªte
    question_fr = "Quels sont les vestiges retrouvÃ©s sur le site nÂ°012 CNSAMH de Sbiba et que rÃ©vÃ¨lent-ils sur lâ€™histoire du lieu ?"
    response_fr = rag.query(question_fr)
    print("\nğŸ” RÃ©ponse experte (FR) :")
    print(response_fr)
"""

question_en = "What archaeological discoveries have been made in Sbiba, and what do they reveal about its ancient history?"
response_en = rag.query(question_en)
print("\nğŸ” Expert answer (EN) :")
print(response_en)
